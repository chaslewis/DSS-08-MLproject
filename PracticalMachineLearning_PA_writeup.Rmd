---
title: Qualitative Classification of Bicep Curl Form from Personal Activity Monitor Data
author: "CEL"
date: "Saturday, February 21, 2015"
output: html_document
---

```{r echo=FALSE}
knitr::opts_chunk$set(echo=TRUE, comment='', message=FALSE, warning=FALSE)
```

## Abstract

Growth of the [Quantified Self](http://en.wikipedia.org/wiki/Quantified_Self) movement has been spurred by the widespread availability of personal [activity trackers](http://en.wikipedia.org/wiki/Activity_tracker) including such devices as the Fitbit, Jawbone, and Nike Fuelband, which use physical orientation and acceleration sensors to gather data on the daily activity of an individual.  One simple popular metric is the number of steps taken in a day; there are even friendly competitions within on-line social groups to see who can accumulate the most steps.

[A research group within the Human Activity Recognition project of Groupware@LES](http://groupware.les.inf.puc-rio.br/har) has undertaken a study, using customized activity monitors, to determine if the quality (correctness of form) of weightlifing exercises can be assessed.  The work we are doing here takes some raw data from that study that was generated by a sample of six individuals performing a set of ten repetions of a one-armed dumbell curl.  As explained in the original researcher's [paper](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201), the form of execution is categorized in five classes, one of which is deemed correct and the others deviating from recommended form in some specific way:

Class |Description |
------|-------------
A     |correct execution 
B     |throwing the elbows to the front
C     |lifting the dumbbell only halfway
D     |lowering the dumbbell only halfway
E     |throwing the hips to the front 

Each row in the dataset includes a large number of observations of measurements generated by the monitoring hardware; these are not completely documented in the researchers' report.  Each row in the dataset is assigned to one of the five classes.  Our goal here is to select and train a Machine Learning (ML) algorithm to correctly assign the class to novel (peviously unseen) sets of observations.  One can imagine such a capability finding practical use as a virtual personal trainer, guiding its users in the proper execution of fitness routines.

All analyses will be performed with the R software, with added libraries to enable our approach as noted.

The dataset is used under the [CC BY-SA](https://creativecommons.org/licenses/by-sa/4.0/) license povided by Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
We gratefully acknowledge the generous contribution of this dataset to the public domain, and to our efforts at learning Data Science.

## Loading, Exploring, and Preprocessing the Data

The dataset is provided as a CSV text file and is imported into R (with some practical optimizations to avoid downloading ane importing more than once).  

```{r}

if (file.exists("pml-training.rds")) {
    trainraw <- readRDS("pml-training.rds")
} else {
    if (!file.exists("pml-training.csv"))
        download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                  destfile="pml-training.csv")
    trainraw <- read.csv("pml-training.csv")
    # serialize object to file so it can be more quickly reloaded:
    saveRDS(trainraw, "pml-training.rds")
}    

trainraw.dim <- dim(trainraw)
trainraw.dim
```

Preliminary examination of the dataset establishes that there are many columns (out of the **$`r trainraw.dim[2]`$**) that are empty in most of the **$`r trainraw.dim[1]`$** rows. We will remove these columns from further consideration since it seems highly unlikely that we could achieve a more accurate result by attempting to interpolate or "plug" the missing values in the absence of much more detailed information about what these observations represent.

```{r}
if (!file.exists("pml-testing.csv")) {
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
        destfile="pml-testing.csv")
}    
testraw <- read.csv("pml-testing.csv")
# won't bother saving to rds since it's so small

testraw.dim <- dim(testraw)
testraw.dim

```
Note that there is also a small test set provided: **$`r testraw.dim[1]`$** rows of **$`r testraw.dim[2]`$** columns; the 'classe' column is missing in this set, replaced by a "problem_id" which will allow for a truly independent test of our algorithm.

```{r}
# find columns with NA and empty entries
badcols <- which(lapply(trainraw, function(x){sum(is.na(x) | x=="")}) > 0)
# remove those bad columns to make a clean training dataset
library(dplyr)
trainclean <- trainraw %>% select(-badcols, -X, -(raw_timestamp_part_1:num_window))
testclean <- testraw %>% select(-badcols, -X, -(raw_timestamp_part_1:num_window))

# annoying little issue to clean up: some cols wind up as int, numeric expected
testclean <- testclean %>% mutate(magnet_dumbbell_z = as.numeric(magnet_dumbbell_z),
                                  magnet_forearm_y = as.numeric(magnet_forearm_y),
                                  magnet_forearm_z = as.numeric(magnet_forearm_z))
# obtained from correctly submitted results:
testclasse <- c('B', 'A', 'B', 'A', 'A', 'E', 'D', 'B', 'A', 'A', 'B', 'C', 'B', 'A', 'E', 'E', 'A', 'B', 'B', 'B')


trainclean.dim <- dim(trainclean)
trainclean.dim
sum(complete.cases(trainclean))
table(trainclean$user_name, trainclean$classe)

```

So, after removing mostly empty columns, there are **$`r trainclean.dim[2]`$** columns remaining in the dataset, all of whose **$`r trainclean.dim[1]`$** rows are complete.  We note that the observations are taken from six individuals.

We will now partition the training dataset in two: an actual training set from which our algorithm will be built, and a cross-validation set against which our algorithm will be tested.  By validating against data that were not seen during algorithm building, we will obtain a credible estimate of the out-of-sample error that can be expected.

The random partitioning will be 90% to the training set and 10% to cross-validation, using a utility from the 'caret' package.

```{r}
library(caret)
set.seed(1234)
inTrain <- createDataPartition(y=trainclean$classe, p=0.9, list=FALSE)

trainset <- trainclean[inTrain,]
xvalset <- trainclean[-inTrain,]
trainset.dim <- dim(trainset)
trainset.dim
xvalset.dim <- dim(xvalset)
xvalset.dim
```


## Building an ML Algorithm

Now that we have clean training (**$`r trainset.dim[1]`$** rows) and cross-validation () **$`r xvalset.dim[1]`$** rows) sets, we are ready to tackle the ML problem by applying standard-practice classification algorithms.

### Classification Tree: modest to good success

First, we will attempt to fit a classification tree model using the 'rpart' package; we will start with this relatively simple algorithm and add complexity if and as needed. 

```{r cache=TRUE}
library(rpart)

model03 <- rpart(classe ~ ., method="class", data=trainset)

pred03t <- predict(model03, newdata=select(trainset, -classe), type="class")
acc03t <- sum(pred03t == trainset$classe)/nrow(trainset)
acc03t 

pred03x <- predict(model03, newdata=select(xvalset, -classe), type="class")
acc03x <- sum(pred03x == xvalset$classe)/nrow(xvalset)
acc03x

```
The in-sample accuraccy of this model is **$`r acc03t `$** while the out-of-sample accuracy (applied to the cross-validation set) is **$`r acc03x `$**.  These results would be quite good in some applications but will try to improve on the out-of-sample error rate of **$`r 1 - acc03x `$**.

Next, we try the classification tree implementation from package 'party'.

```{r cache=TRUE}
library(party)
model02 <- ctree(classe ~ ., data=trainset)

pred02t <- predict(model02, newdata=select(trainset, -classe), type="response")
acc02t <- sum(pred02t == trainset$classe)/nrow(trainset)
acc02t

pred02x <- predict(model02, newdata=select(xvalset, -classe), type="response")
acc02x <- sum(pred02x == xvalset$classe)/nrow(xvalset)
acc02x

```
Interstingly, this package gives much improved results: **$`r acc02t `$** for the trainng accuracy and **$`r acc02x `$** for cross-validation.  We now have a respectible out-of-sample error rate of **$`r 1 - acc02x `$**, but will continue searching for improvement.

### Random Forest: a powerful approach

Next, we apply the [Random Forest ](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm) which leverages multiple classification trees in an ensemble approach; the implementaiton is is package 'randomForest'.

```{r cache=TRUE}
library(randomForest)
model04 <- randomForest(select(trainset, -classe), y=trainset$classe)

pred04t <- predict(model04, select(trainset, -classe))
acc04t <- sum(pred04t == trainset$classe)/nrow(trainset)
acc04t
model04$confusion
pred04x <-  predict(model04, select(xvalset, -classe))
acc04x <- sum(pred04x == xvalset$classe)/nrow(xvalset)
acc04x
```
The astonishing in-sample accuracy of the model (perfect classifiction) might lead us to worry about overfitting.  However, the out-of-sample accuracy of **$`r acc04x`$** confirms that we have a powerful tool on our hands.  The out-of-sample error rate of **$`r 1 - acc04x`$** should be more than sufficient for our purposes going forward.

####Influence of Individual User

The cross-validation being performed here is against a random subset of an original training set that contains a uniform mix of data from all six of the subjects participating.  While the model is being trained and cross-validated against different data, it is significant that data from the same subjects appear in both training and cross-validation datasets.  The issue is not that the user_name column appears in the training set; in fact, omitting it hardly changes the accuracy.  Rather, the interesting case occurs when we try to cross-validate against an individual who was not part of training.  We tried that here:

```{r cache=TRUE}
xch <- filter(trainset, user_name != "charles")
wch <- filter(trainset, user_name == "charles")
model06 <- randomForest(select(xch, -classe, -user_name), y=xch$classe)
pred06t <- predict(model06, select(xch, -classe, -user_name))
acc06t <- sum(pred06t == xch$classe)/nrow(xch)
acc06t
pred06x <-  predict(model06, select(xvalset, -classe))
acc06x <- sum(pred06x == xvalset$classe)/nrow(xvalset)
acc06x
prec06c <- predict(model06, select(wch, -classe, -user_name))
acc06c <- sum(prec06c == wch$classe)/nrow(wch)
acc06c
```

By leaving out one of the users in the training set, the cross-validation accuracy takes a hit, now **$`r acc06x`$**.  The out-of-sample error of **$`r 1 - acc06x`$** is almost entirely due to the rows from the user left out of the training set.  And when we cross-validate against the left out user--with a model that did not train against any data generated by him--the accuracy plunges to **$`r acc06c`$**.  The out-of-sample error rate for novel individuals is a disappointing **$`r 1 - acc06c`$**.  The original researchers' paper (Sec. 5.2) suggests that much larger datasets would be needed for a random forest model to properly new users' activities with an acceptible error rate.  They shifted to another approach which they call "model based"; assessing that direction of inquiry is beyond the scope of this work.


## Concluding Remarks

advantages/disad of ML -- dont have/need intuition for underlying process